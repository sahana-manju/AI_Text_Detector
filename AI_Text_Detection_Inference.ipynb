{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6901731b",
   "metadata": {},
   "source": [
    "## AI Text Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b402b8",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b799b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55875f",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500fffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sahanamanjunath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sahanamanjunath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Make sure to download the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.spell = SpellChecker()\n",
    "\n",
    "    def word_length(self, tex):\n",
    "        all_words = [len(word) for word in tex.split()]\n",
    "        return np.mean(all_words), np.median(all_words)\n",
    "\n",
    "    def word_length2(self, tex):\n",
    "        all_words = [len(word) for word in nltk.tokenize.word_tokenize(tex) if word not in string.punctuation]\n",
    "        return np.mean(all_words), np.median(all_words)\n",
    "\n",
    "    def count_numbers(self, tex):\n",
    "        # Regular expression to match integers and floating-point numbers\n",
    "        numbers = re.findall(r'\\b\\d+(\\.\\d+)?\\b', tex)\n",
    "        return len(numbers)\n",
    "\n",
    "    def count_proper_nouns(self, tex):\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(tex)\n",
    "        # POS tagging\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        # Count proper nouns (NNP, NNPS)\n",
    "        proper_nouns = [word for word, tag in tagged if tag in ('NNP', 'NNPS')]\n",
    "        return len(proper_nouns)\n",
    "\n",
    "    def check_typos_and_replace(self, tex):\n",
    "        # Remove hyphenated words using regex (i.e., words with a hyphen)\n",
    "        tex_clean = re.sub(r'\\b\\w+-\\w+\\b', '', tex)  # Removes words like 'car-centric', 'car-infested', etc.\n",
    "        tex_clean = re.sub(r'\\w+\\'s\\b', '', tex_clean)  # Removes possessive 's (e.g., John's, car's)\n",
    "        # Remove digits (e.g., 123, 456, 789)\n",
    "        tex_clean = re.sub(r'\\d+', '', tex_clean)\n",
    "        # Remove words that end with a period (e.g., \"word.\" becomes \"word\")\n",
    "        tex_clean = re.sub(r'\\b\\w+\\.\\b', '', tex_clean)\n",
    "        # Remove punctuation\n",
    "        tex_clean = re.sub(r'[^\\w\\s]', '', tex_clean)\n",
    "\n",
    "        # Tokenize the cleaned text\n",
    "        words = nltk.tokenize.word_tokenize(tex_clean)\n",
    "\n",
    "        # POS tagging to identify proper nouns\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "        # Filter out proper nouns (NNP and NNPS tags are for singular and plural proper nouns)\n",
    "        words_without_proper_nouns = [word for word, tag in tagged_words if tag not in ('NNP', 'NNPS')]\n",
    "\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        words_filtered = [word.lower() for word in words_without_proper_nouns if word not in string.punctuation]\n",
    "\n",
    "        # Find all the misspelled words\n",
    "        misspelled = self.spell.unknown(words_filtered)\n",
    "\n",
    "        # Replace the misspelled words with \"TYPO\"\n",
    "        words_with_typos_replaced = [\n",
    "            'TYPO' if word.lower() in misspelled else word\n",
    "            for word in nltk.tokenize.word_tokenize(tex)\n",
    "        ]\n",
    "        \n",
    "        # Reconstruct the text after replacing the misspelled words\n",
    "        tex_with_typos_replaced = ' '.join(words_with_typos_replaced)\n",
    "\n",
    "        return tex_with_typos_replaced\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        print(\"Starting preprocessing...\")\n",
    "        \n",
    "        # Get text length (character count)\n",
    "        df['text_length'] = df['text'].str.len()\n",
    "        \n",
    "        # Get word lengths (mean, max)\n",
    "        df['mean_word_length'] = df['text'].map(lambda tex: np.mean([len(word) for word in tex.split()]))\n",
    "        df['max_word_length'] = df['text'].map(lambda tex: np.max([len(word) for word in tex.split()]))\n",
    "        \n",
    "        # Count unique words\n",
    "        df['unique_word_count'] = df['text'].map(lambda tex: len(set([word.lower() for word in nltk.tokenize.word_tokenize(tex) if word not in string.punctuation])))\n",
    "        \n",
    "        # Sentence length analysis\n",
    "        df['sentences'] = df['text'].map(lambda tex: nltk.sent_tokenize(tex))  # Using nltk method\n",
    "        df['sentence_length'] = df['sentences'].map(lambda x: len(x))\n",
    "        df['mean_sentence'] = df['sentences'].map(lambda x: np.mean([len(i) for i in x]))\n",
    "        \n",
    "        # Count numbers\n",
    "        df['number_count'] = df['text'].map(self.count_numbers)\n",
    "        \n",
    "        # Count proper nouns\n",
    "        df['proper_noun_count'] = df['text'].map(self.count_proper_nouns)\n",
    "        \n",
    "        # Correct typos\n",
    "        df['text_with_typos_replaced'] = df['text'].map(self.check_typos_and_replace)\n",
    "        \n",
    "        print(\"Preprocessing completed.\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f35fb1",
   "metadata": {},
   "source": [
    "### Loading tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d5cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahanamanjunath/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['bert.embeddings.position_ids'], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the Dataset class (same as you used in training)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['text_with_typos_replaced'].tolist()\n",
    "        self.labels = df['generated'].tolist()\n",
    "        self.features = df[['text_length', 'mean_word_length', 'sentence_length', 'mean_sentence', \n",
    "                            'unique_word_count', 'proper_noun_count', 'number_count']].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return input_ids, attention_mask, features, label\n",
    "\n",
    "# Define the model (same as you used in training)\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768 + num_features, 2)  # 768 for BERT hidden size, + num_features\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, numerical_features):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        combined_input = torch.cat((pooled_output, numerical_features), dim=1)\n",
    "        x = self.dropout(self.relu(combined_input))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load the BERT part of the model weights\n",
    "model = BertClassifier(num_features=7)  # 7 features as per your dataset\n",
    "\n",
    "# Load BERT model weights\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.bert.load_state_dict(bert_model.state_dict(), strict=False)\n",
    "\n",
    "# Now, load the weights for the custom layers (e.g., fully connected layer)\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\", map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975e3c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=775, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to device (now it should be on CPU)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148ecfb",
   "metadata": {},
   "source": [
    "### Inference based on single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9bf0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Sample test essay (replace this with your actual test essay)\n",
    "sample_essay = \"Artificial Intelligence has become soo popular now! I remember doing all my homeworks myself early 2000s but now I take help of GPT\"\n",
    "\n",
    "# Convert the essay into a DataFrame with a 'text' column\n",
    "test_df = pd.DataFrame({'text': [sample_essay]})\n",
    "\n",
    "# Instantiate the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Run preprocessing on the single essay\n",
    "processed_test_df = preprocessor.preprocess_data(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bb80c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>max_word_length</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>mean_sentence</th>\n",
       "      <th>number_count</th>\n",
       "      <th>proper_noun_count</th>\n",
       "      <th>text_with_typos_replaced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence has become soo popular...</td>\n",
       "      <td>131</td>\n",
       "      <td>4.73913</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>[Artificial Intelligence has become soo popula...</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Artificial Intelligence has become soo popular...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_length  \\\n",
       "0  Artificial Intelligence has become soo popular...          131   \n",
       "\n",
       "   mean_word_length  max_word_length  unique_word_count  \\\n",
       "0           4.73913               12                 21   \n",
       "\n",
       "                                           sentences  sentence_length  \\\n",
       "0  [Artificial Intelligence has become soo popula...                2   \n",
       "\n",
       "   mean_sentence  number_count  proper_noun_count  \\\n",
       "0           65.0             0                  2   \n",
       "\n",
       "                            text_with_typos_replaced  \n",
       "0  Artificial Intelligence has become soo popular...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5f3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetTest(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Extract the same columns as the training set, but without labels\n",
    "        self.texts = df['text_with_typos_replaced'].tolist()\n",
    "        self.features = df[['text_length', 'mean_word_length', 'sentence_length', 'mean_sentence', \n",
    "                            'unique_word_count', 'proper_noun_count', 'number_count']].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text and convert it to input format\n",
    "        encoding = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Convert the numerical features to tensor\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Return the inputs (input_ids, attention_mask, features) without labels\n",
    "        return input_ids, attention_mask, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247a00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "val_dataset = TextDatasetTest(processed_test_df)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a06d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to classify a single test sample (one row of text)\n",
    "def classify_single_sample(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Initialize variables for classification\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        # Wrap test_loader with tqdm for progress bar\n",
    "        progress_bar = tqdm(test_loader, desc=\"Classifying Sample\", leave=True)\n",
    "\n",
    "        for input_ids, attention_mask, numerical_features in progress_bar:\n",
    "            input_ids, attention_mask, numerical_features = (\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                numerical_features.to(device),\n",
    "            )\n",
    "\n",
    "            # Forward pass to get the model's output\n",
    "            outputs = model(input_ids, attention_mask, numerical_features)\n",
    "            \n",
    "            # Get the predicted class (0 for human, 1 for AI, assuming binary classification)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.append(predicted.item())\n",
    "\n",
    "            # Update the progress bar with the latest prediction\n",
    "            progress_bar.set_postfix(prediction=predicted.item())\n",
    "\n",
    "    # Output the final prediction for the single sample\n",
    "    if predictions:\n",
    "        print(f\"Prediction: {'AI' if predictions[0] == 1 else 'Human'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90abb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Sample: 100%|███████████| 1/1 [00:00<00:00,  5.85it/s, prediction=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the classification function on the test data\n",
    "classify_single_sample(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
